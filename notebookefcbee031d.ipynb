{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13218908,"sourceType":"datasetVersion","datasetId":8378799}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models, losses, optimizers, callbacks\nimport os\n\ntrain_dir = '/kaggle/input/kidney-disease-analytica/analytica_PS/train'\ntest_dir = '/kaggle/input/kidney-disease-analytica/analytica_PS/test'\n\nIMG_SIZE = 128\nBATCH_SIZE = 32\nEPOCHS = 20\nLEARNING_RATE = 0.001\nWEIGHT_DECAY = 1e-4\nMODEL_SAVE_PATH = 'best_kidney_stone_cnn.h5'\n\ntrain_dataset = tf.keras.utils.image_dataset_from_directory(\n    train_dir,\n    labels='inferred',\n    label_mode='int',\n    image_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,\n    shuffle=True\n)\n\ntest_dataset = tf.keras.utils.image_dataset_from_directory(\n    test_dir,\n    labels='inferred',\n    label_mode='int',\n    image_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,\n    shuffle=False\n)\n\nclass_names = train_dataset.class_names\nnum_classes = len(class_names)\nprint(f\"Found classes: {class_names}\")\n\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\ntest_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)\n\ndata_augmentation = models.Sequential([\n    layers.RandomFlip(\"horizontal\"),\n    layers.RandomRotation(0.1),\n], name='data_augmentation')\n\nmodel = models.Sequential([\n    layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3)),\n    data_augmentation,\n    layers.Rescaling(1./255),\n    layers.Conv2D(16, kernel_size=3, padding='same', activation='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D(pool_size=2),\n    layers.Conv2D(32, kernel_size=3, padding='same', activation='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D(pool_size=2),\n    layers.Conv2D(64, kernel_size=3, padding='same', activation='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D(pool_size=2),\n    layers.Flatten(),\n    layers.Dense(512, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(num_classes)\n])\n\nmodel.summary()\n\nloss_function = losses.SparseCategoricalCrossentropy(from_logits=True)\noptimizer = optimizers.Adam(learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n\ndef lr_scheduler(epoch, lr):\n    if (epoch + 1) % 7 == 0 and epoch > 0:\n        return lr * 0.1\n    return lr\n\nlr_callback = callbacks.LearningRateScheduler(lr_scheduler)\n\ncheckpoint_callback = callbacks.ModelCheckpoint(\n    filepath=MODEL_SAVE_PATH,\n    monitor='val_accuracy',\n    save_best_only=True,\n    mode='max',\n    verbose=1\n)\n\nmodel.compile(\n    optimizer=optimizer,\n    loss=loss_function,\n    metrics=['accuracy']\n)\n\nprint(\"Starting training...\")\nhistory = model.fit(\n    train_dataset,\n    epochs=EPOCHS,\n    validation_data=test_dataset,\n    callbacks=[checkpoint_callback, lr_callback]\n)\nprint(\"Finished Training\")\n\nbest_val_acc = max(history.history['val_accuracy'])\nprint(f'Best Validation Accuracy achieved during training: {best_val_acc * 100:.2f}%')\n\nprint(f\"Best model '{MODEL_SAVE_PATH}' loaded for final evaluation.\")\nbest_model = models.load_model(MODEL_SAVE_PATH)\n\nloss, final_accuracy = best_model.evaluate(test_dataset)\n\nprint(f'Final accuracy of the best model on the test images: {final_accuracy * 100:.2f}%')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models, losses, optimizers, callbacks\nimport os\n\n# --- 1. SETUP AND HYPERPARAMETERS ---\n\n\ntrain_dir = '/kaggle/input/kidney-disease-analytica/analytica_PS/train'\ntest_dir = '/kaggle/input/kidney-disease-analytica/analytica_PS/test'\n\n# Hyperparameter adjustments\nIMG_SIZE = 128\nBATCH_SIZE = 32\n# Increased epochs since we are using Early Stopping\nEPOCHS = 50\nLEARNING_RATE = 0.001\nWEIGHT_DECAY = 1e-4\nMODEL_SAVE_PATH = 'best_kidney_stone_cnn_v2.h5'\n\n# --- 2. DATA LOADING ---\n\ntrain_dataset = tf.keras.utils.image_dataset_from_directory(\n    train_dir,\n    labels='inferred',\n    label_mode='int',\n    image_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,\n    shuffle=True\n)\n\ntest_dataset = tf.keras.utils.image_dataset_from_directory(\n    test_dir,\n    labels='inferred',\n    label_mode='int',\n    image_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,\n    shuffle=False\n)\n\nclass_names = train_dataset.class_names\nnum_classes = len(class_names)\nprint(f\"Found classes: {class_names}\")\n\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\ntest_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)\n\n\n# --- 3. IMPROVED DATA AUGMENTATION AND MODEL ARCHITECTURE ---\n\n# Enhanced data augmentation pipeline\ndata_augmentation = models.Sequential([\n    layers.RandomFlip(\"horizontal\"),\n    layers.RandomRotation(0.1),\n    layers.RandomZoom(0.1),\n    layers.RandomTranslation(height_factor=0.1, width_factor=0.1),\n    layers.RandomContrast(0.1),\n], name='data_augmentation')\n\n# Deeper and more robust model architecture\nmodel = models.Sequential([\n    layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3)),\n    data_augmentation,\n    layers.Rescaling(1./255),\n\n    # Block 1\n    layers.Conv2D(32, kernel_size=3, padding='same', activation='relu'),\n    layers.BatchNormalization(),\n    layers.Conv2D(32, kernel_size=3, padding='same', activation='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D(pool_size=2),\n\n    # Block 2\n    layers.Conv2D(64, kernel_size=3, padding='same', activation='relu'),\n    layers.BatchNormalization(),\n    layers.Conv2D(64, kernel_size=3, padding='same', activation='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D(pool_size=2),\n\n    # Block 3\n    layers.Conv2D(128, kernel_size=3, padding='same', activation='relu'),\n    layers.BatchNormalization(),\n    layers.Conv2D(128, kernel_size=3, padding='same', activation='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D(pool_size=2),\n    \n    # Block 4\n    layers.Conv2D(256, kernel_size=3, padding='same', activation='relu'),\n    layers.BatchNormalization(),\n    layers.Conv2D(256, kernel_size=3, padding='same', activation='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D(pool_size=2),\n\n    # Classifier Head\n    layers.GlobalAveragePooling2D(), # More robust than Flatten for image data\n    layers.Dense(512, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(num_classes)\n])\n\nmodel.summary()\n\n# --- 4. IMPROVED TRAINING SETUP ---\n\nloss_function = losses.SparseCategoricalCrossentropy(from_logits=True)\noptimizer = optimizers.Adam(learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n\n# Define a set of callbacks for smarter training\nsmart_callbacks = [\n    # Stop training if `val_accuracy` doesn't improve for 10 epochs\n    callbacks.EarlyStopping(\n        monitor='val_accuracy',\n        patience=10,\n        verbose=1,\n        restore_best_weights=True # Restores model weights from the epoch with the best value\n    ),\n    # Reduce learning rate if `val_accuracy` plateaus for 3 epochs\n    callbacks.ReduceLROnPlateau(\n        monitor='val_accuracy',\n        factor=0.2, # new_lr = lr * factor\n        patience=3,\n        verbose=1,\n        min_lr=1e-6\n    ),\n    # Save the best model automatically\n    callbacks.ModelCheckpoint(\n        filepath=MODEL_SAVE_PATH,\n        monitor='val_accuracy',\n        save_best_only=True,\n        mode='max',\n        verbose=1\n    )\n]\n\nmodel.compile(\n    optimizer=optimizer,\n    loss=loss_function,\n    metrics=['accuracy']\n)\n\n# --- 5. TRAINING AND EVALUATION ---\n\nprint(\"Starting training with improved strategy...\")\nhistory = model.fit(\n    train_dataset,\n    epochs=EPOCHS,\n    validation_data=test_dataset,\n    callbacks=smart_callbacks\n)\nprint(\"Finished Training.\")\n\n# The EarlyStopping callback with `restore_best_weights=True` ensures\n# the model object already has the best weights, so we can evaluate it directly.\nprint(\"\\nEvaluating the best model on the test set...\")\nloss, final_accuracy = model.evaluate(test_dataset)\n\nprint(f'Final accuracy of the best model on the test images: {final_accuracy * 100:.2f}%')","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null}]}